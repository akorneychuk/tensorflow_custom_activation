{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "3.7.8 | packaged by conda-forge | (default, Jul 23 2020, 03:54:19) \n",
      "[GCC 7.5.0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version)\n",
    "\n",
    "\n",
    "def produce_initial_weights(shape):\n",
    "    return (0.2 * np.random.random(shape) - 0.1).reshape(shape).astype('float32')\n",
    "\n",
    "\n",
    "def leaky_relu(x):\n",
    "    if (x < 0):\n",
    "        return 0.01 * x\n",
    "    if (x >= 0 and x <= 1):\n",
    "        return x\n",
    "    if (x > 1):\n",
    "        return 1 + 0.01 * (x - 1)\n",
    "\n",
    "\n",
    "def d_leaky_relu(x):\n",
    "    if (x < 0):\n",
    "        return 0.01\n",
    "    if (x >= 0 and x <= 1):\n",
    "        return 1\n",
    "    if (x > 1):\n",
    "        return 0.01\n",
    "\n",
    "\n",
    "np_leaky_relu = np.vectorize(leaky_relu, otypes=[np.float32])\n",
    "np_d_leaky_relu = np.vectorize(d_leaky_relu, otypes=[np.float32])\n",
    "\n",
    "np_leaky_relu_32 = lambda x: np_leaky_relu(x).astype(np.float32)\n",
    "np_d_leaky_relu_32 = lambda x: np_d_leaky_relu(x).astype(np.float32)\n",
    "\n",
    "\n",
    "def relu_grad(op, grad):\n",
    "    x = op.inputs[0]\n",
    "    r = tf.mod(x, 1)\n",
    "    n_gr = tf.to_float(tf.less_equal(r, 0.5))\n",
    "    grad_res = grad * n_gr\n",
    "    return grad_res\n",
    "\n",
    "\n",
    "def py_func(func, inp, Tout, name=None, grad=None):\n",
    "    # Need to generate a unique name to avoid duplicates:\n",
    "    rnd_name = 'PyFuncGrad' + str(np.random.randint(0, 1E+2))\n",
    "    tf.RegisterGradient(rnd_name)(grad)\n",
    "    g = tf.Graph()\n",
    "    with g.gradient_override_map({\"PyFunc\": rnd_name}):\n",
    "        return tf.py_function(func, inp, Tout, name)\n",
    "\n",
    "\n",
    "def tf_leaky_relu(x,name=None):\n",
    "    with ops.name_scope(name, \"d_spiky\", [x]) as name:\n",
    "        y = py_func(np_leaky_relu_32,  # forward pass function\n",
    "                    [x],\n",
    "                    [tf.float32],\n",
    "                    name=name,\n",
    "                    grad=relu_grad)  # the function that overrides gradient\n",
    "        return y[0]\n",
    "\n",
    "\n",
    "def tf_d_leaky_relu(x,name=None):\n",
    "    with ops.name_scope(name, \"d_leaky_relu\", [x]) as name:\n",
    "        y = py_func(np_d_leaky_relu_32,\n",
    "                    [x],\n",
    "                    [tf.float32],\n",
    "                    name=name)\n",
    "        return y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "def mse(true, predicted):\n",
    "    mse_res = tf.reduce_mean(tf.square(true - predicted))\n",
    "    return mse_res\n",
    "\n",
    "hidden_layer_1_size = 12\n",
    "hidden_layer_2_size = 10\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "inputs = np.array([[1, 0, 1, 1, 0],\n",
    "                   [0, 0, 1, 0, 1],\n",
    "                   [0, 1, 1, 1, 1],\n",
    "                   [1, 0, 0, 0, 0],\n",
    "                   [1, 1, 0, 1, 0],\n",
    "                   [0, 1, 1, 0, 0],\n",
    "                   [1, 1, 1, 1, 1],\n",
    "                   [1, 0, 0, 0, 1]]).astype('float32')\n",
    "\n",
    "weights_0_1 = tf.Variable(produce_initial_weights([inputs.shape[1], hidden_layer_1_size]))\n",
    "bias_0_1 = tf.Variable(produce_initial_weights([1, hidden_layer_1_size]))\n",
    "weights_1_2 = tf.Variable(produce_initial_weights([hidden_layer_1_size, hidden_layer_2_size]))\n",
    "bias_1_2 = tf.Variable(produce_initial_weights([1, hidden_layer_2_size]))\n",
    "weights_2_3 = tf.Variable(produce_initial_weights([hidden_layer_2_size, 1]))\n",
    "expects = tf.constant(np.array([1, 1, 0, 0, 1, 0, 0, 1]).astype('float32'))\n",
    "\n",
    "cached_weigts = []\n",
    "error_holder = []\n",
    "\n",
    "for j in range(3000):\n",
    "    error, correct_cnt = (0.0, 0)\n",
    "    full_iteration_error = 0\n",
    "    for i in range(len(expects)):\n",
    "        input = tf.reshape(tf.constant(inputs[i]), [1, 5])\n",
    "        expect = tf.reshape(tf.constant(expects[i]), [1, 1])\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as t:\n",
    "            t.watch(weights_0_1)\n",
    "            t.watch(bias_0_1)\n",
    "            t.watch(weights_1_2)\n",
    "            t.watch(bias_1_2)\n",
    "            t.watch(weights_2_3)\n",
    "\n",
    "            hidden_1 = tf_leaky_relu(tf.add(tf.matmul(input, weights_0_1), bias_0_1))\n",
    "            hidden_2 = tf_leaky_relu(tf.add(tf.matmul(hidden_1, weights_1_2), bias_1_2))\n",
    "            predict = tf_leaky_relu(tf.matmul(hidden_2, weights_2_3))\n",
    "\n",
    "            # hidden_1 = tf.nn.relu(tf.add(tf.matmul(input, weights_0_1), bias_0_1))\n",
    "            # hidden_2 = tf.nn.relu(tf.add(tf.matmul(hidden_1, weights_1_2), bias_1_2))\n",
    "            # predict = tf.nn.relu(tf.matmul(hidden_2, weights_2_3))\n",
    "\n",
    "            loss = mse(predict, expect)\n",
    "\n",
    "        grads = t.gradient(loss, [weights_0_1, bias_0_1, weights_1_2, bias_1_2, weights_2_3])\n",
    "        optimizer = tf.optimizers.Adam(0.01)\n",
    "        optimizer.apply_gradients(zip(grads, [weights_0_1, bias_0_1, weights_1_2, bias_1_2, weights_2_3]))\n",
    "\n",
    "        error += np.sum(loss)\n",
    "        correct_cnt += int(np.argmax(predict) == np.argmax(expect))\n",
    "\n",
    "        full_iteration_error = np.sum((predict - expect) ** 2)\n",
    "\n",
    "    weights_0_1_current = weights_0_1.value().numpy().copy()\n",
    "    weights_1_2_current = weights_1_2.value().numpy().copy()\n",
    "\n",
    "    if j % 100 == 0:\n",
    "        cached_weigts.append({\n",
    "            \"error\": error,\n",
    "            \"weigts\": {\n",
    "                \"weights_0_1\": weights_0_1_current,\n",
    "                \"weights_1_2\": weights_1_2_current\n",
    "            }\n",
    "        })\n",
    "\n",
    "        print(\"Error: \", error)\n",
    "        print(\"Correct cnt: \", correct_cnt)\n",
    "\n",
    "print(\"END\")\n",
    "print(\"cached_weigts: \", cached_weigts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
